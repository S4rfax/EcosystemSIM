{
    "name": "root",
    "gauges": {
        "MyBehavior.Policy.Entropy.mean": {
            "value": 1.4382883310317993,
            "min": 1.4189382791519165,
            "max": 1.4388976097106934,
            "count": 25
        },
        "MyBehavior.Policy.Entropy.sum": {
            "value": 14598.626953125,
            "min": 13726.857421875,
            "max": 15892.109375,
            "count": 25
        },
        "MyBehavior.Step.mean": {
            "value": 249955.0,
            "min": 9970.0,
            "max": 249955.0,
            "count": 25
        },
        "MyBehavior.Step.sum": {
            "value": 249955.0,
            "min": 9970.0,
            "max": 249955.0,
            "count": 25
        },
        "MyBehavior.Policy.ExtrinsicValueEstimate.mean": {
            "value": 21.278226852416992,
            "min": -0.14850561320781708,
            "max": 21.278226852416992,
            "count": 25
        },
        "MyBehavior.Policy.ExtrinsicValueEstimate.sum": {
            "value": 3787.5244140625,
            "min": -23.463886260986328,
            "max": 3787.5244140625,
            "count": 25
        },
        "MyBehavior.Environment.EpisodeLength.mean": {
            "value": 229.29268292682926,
            "min": 229.29268292682926,
            "max": 434.96,
            "count": 25
        },
        "MyBehavior.Environment.EpisodeLength.sum": {
            "value": 9401.0,
            "min": 1900.0,
            "max": 11552.0,
            "count": 25
        },
        "MyBehavior.Environment.CumulativeReward.mean": {
            "value": 87.6829268292683,
            "min": -10.0,
            "max": 102.8048780487805,
            "count": 25
        },
        "MyBehavior.Environment.CumulativeReward.sum": {
            "value": 3595.0,
            "min": -295.0,
            "max": 4215.0,
            "count": 25
        },
        "MyBehavior.Policy.ExtrinsicReward.mean": {
            "value": 87.6829268292683,
            "min": -10.0,
            "max": 102.8048780487805,
            "count": 25
        },
        "MyBehavior.Policy.ExtrinsicReward.sum": {
            "value": 3595.0,
            "min": -295.0,
            "max": 4215.0,
            "count": 25
        },
        "MyBehavior.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 25
        },
        "MyBehavior.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 25
        },
        "CarnivoreBehavior.Policy.Entropy.mean": {
            "value": 1.4575974941253662,
            "min": 1.4183827638626099,
            "max": 1.4575974941253662,
            "count": 25
        },
        "CarnivoreBehavior.Policy.Entropy.sum": {
            "value": 14794.6142578125,
            "min": 13722.853515625,
            "max": 15892.109375,
            "count": 25
        },
        "CarnivoreBehavior.Step.mean": {
            "value": 249979.0,
            "min": 9941.0,
            "max": 249979.0,
            "count": 25
        },
        "CarnivoreBehavior.Step.sum": {
            "value": 249979.0,
            "min": 9941.0,
            "max": 249979.0,
            "count": 25
        },
        "CarnivoreBehavior.Policy.ExtrinsicValueEstimate.mean": {
            "value": -0.6288454532623291,
            "min": -2.3546032905578613,
            "max": -0.4186165928840637,
            "count": 25
        },
        "CarnivoreBehavior.Policy.ExtrinsicValueEstimate.sum": {
            "value": -113.19218444824219,
            "min": -414.4101867675781,
            "max": -74.09513854980469,
            "count": 25
        },
        "CarnivoreBehavior.Environment.EpisodeLength.mean": {
            "value": 215.5909090909091,
            "min": 215.5909090909091,
            "max": 406.88461538461536,
            "count": 25
        },
        "CarnivoreBehavior.Environment.EpisodeLength.sum": {
            "value": 9486.0,
            "min": 2316.0,
            "max": 11584.0,
            "count": 25
        },
        "CarnivoreBehavior.Environment.CumulativeReward.mean": {
            "value": -4.7727272727272725,
            "min": -13.461538461538462,
            "max": -1.1904761904761905,
            "count": 25
        },
        "CarnivoreBehavior.Environment.CumulativeReward.sum": {
            "value": -210.0,
            "min": -370.0,
            "max": -45.0,
            "count": 25
        },
        "CarnivoreBehavior.Policy.ExtrinsicReward.mean": {
            "value": -4.7727272727272725,
            "min": -13.461538461538462,
            "max": -1.1904761904761905,
            "count": 25
        },
        "CarnivoreBehavior.Policy.ExtrinsicReward.sum": {
            "value": -210.0,
            "min": -370.0,
            "max": -45.0,
            "count": 25
        },
        "CarnivoreBehavior.IsTraining.mean": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 25
        },
        "CarnivoreBehavior.IsTraining.sum": {
            "value": 1.0,
            "min": 1.0,
            "max": 1.0,
            "count": 25
        },
        "MyBehavior.Losses.PolicyLoss.mean": {
            "value": 0.024372727596589053,
            "min": 0.018057694403008404,
            "max": 0.026170850937099508,
            "count": 24
        },
        "MyBehavior.Losses.PolicyLoss.sum": {
            "value": 0.024372727596589053,
            "min": 0.018057694403008404,
            "max": 0.026170850937099508,
            "count": 24
        },
        "MyBehavior.Losses.ValueLoss.mean": {
            "value": 66.94833628336589,
            "min": 3.6125624815622968,
            "max": 78.86851170857747,
            "count": 24
        },
        "MyBehavior.Losses.ValueLoss.sum": {
            "value": 66.94833628336589,
            "min": 3.6125624815622968,
            "max": 78.86851170857747,
            "count": 24
        },
        "MyBehavior.Policy.LearningRate.mean": {
            "value": 0.0002627217124261001,
            "min": 0.0002627217124261001,
            "max": 0.00029834130055289993,
            "count": 24
        },
        "MyBehavior.Policy.LearningRate.sum": {
            "value": 0.0002627217124261001,
            "min": 0.0002627217124261001,
            "max": 0.00029834130055289993,
            "count": 24
        },
        "MyBehavior.Policy.Epsilon.mean": {
            "value": 0.18757390000000004,
            "min": 0.18757390000000004,
            "max": 0.1994471000000001,
            "count": 24
        },
        "MyBehavior.Policy.Epsilon.sum": {
            "value": 0.18757390000000004,
            "min": 0.18757390000000004,
            "max": 0.1994471000000001,
            "count": 24
        },
        "MyBehavior.Policy.Beta.mean": {
            "value": 0.00437993761,
            "min": 0.00437993761,
            "max": 0.0049724102900000015,
            "count": 24
        },
        "MyBehavior.Policy.Beta.sum": {
            "value": 0.00437993761,
            "min": 0.00437993761,
            "max": 0.0049724102900000015,
            "count": 24
        },
        "CarnivoreBehavior.Losses.PolicyLoss.mean": {
            "value": 0.027151536118860047,
            "min": 0.016890580372031158,
            "max": 0.04120508876318733,
            "count": 24
        },
        "CarnivoreBehavior.Losses.PolicyLoss.sum": {
            "value": 0.027151536118860047,
            "min": 0.016890580372031158,
            "max": 0.04120508876318733,
            "count": 24
        },
        "CarnivoreBehavior.Losses.ValueLoss.mean": {
            "value": 1.4310882210731506,
            "min": 0.3416443741569916,
            "max": 8.825096599260966,
            "count": 24
        },
        "CarnivoreBehavior.Losses.ValueLoss.sum": {
            "value": 1.4310882210731506,
            "min": 0.3416443741569916,
            "max": 8.825096599260966,
            "count": 24
        },
        "CarnivoreBehavior.Policy.LearningRate.mean": {
            "value": 0.0002627277124240999,
            "min": 0.0002627277124240999,
            "max": 0.00029835525054825005,
            "count": 24
        },
        "CarnivoreBehavior.Policy.LearningRate.sum": {
            "value": 0.0002627277124240999,
            "min": 0.0002627277124240999,
            "max": 0.00029835525054825005,
            "count": 24
        },
        "CarnivoreBehavior.Policy.Epsilon.mean": {
            "value": 0.18757589999999996,
            "min": 0.18757589999999996,
            "max": 0.19945174999999996,
            "count": 24
        },
        "CarnivoreBehavior.Policy.Epsilon.sum": {
            "value": 0.18757589999999996,
            "min": 0.18757589999999996,
            "max": 0.19945174999999996,
            "count": 24
        },
        "CarnivoreBehavior.Policy.Beta.mean": {
            "value": 0.00438003741,
            "min": 0.00438003741,
            "max": 0.004972642325,
            "count": 24
        },
        "CarnivoreBehavior.Policy.Beta.sum": {
            "value": 0.00438003741,
            "min": 0.00438003741,
            "max": 0.004972642325,
            "count": 24
        }
    },
    "metadata": {
        "timer_format_version": "0.1.0",
        "start_time_seconds": "1736180289",
        "python_version": "3.10.12 (main, Jul  5 2023, 15:34:07) [Clang 14.0.6 ]",
        "command_line_arguments": "/Users/luis/opt/anaconda3/envs/mlagents/bin/mlagents-learn config/multitraining.yaml --run-id=Multitraining3",
        "mlagents_version": "1.1.0",
        "mlagents_envs_version": "1.1.0",
        "communication_protocol_version": "1.5.0",
        "pytorch_version": "2.2.2",
        "numpy_version": "1.23.5",
        "end_time_seconds": "1736181068"
    },
    "total": 779.1170731249731,
    "count": 1,
    "self": 0.009032958885654807,
    "children": {
        "run_training.setup": {
            "total": 0.03146779106464237,
            "count": 1,
            "self": 0.03146779106464237
        },
        "TrainerController.start_learning": {
            "total": 779.0765723750228,
            "count": 1,
            "self": 0.22591345652472228,
            "children": {
                "TrainerController._reset_env": {
                    "total": 16.419746791943908,
                    "count": 1,
                    "self": 16.419746791943908
                },
                "TrainerController.advance": {
                    "total": 762.1404672504868,
                    "count": 10928,
                    "self": 0.2526075068162754,
                    "children": {
                        "env_step": {
                            "total": 651.3507073740475,
                            "count": 10928,
                            "self": 626.796314391424,
                            "children": {
                                "SubprocessEnvManager._take_step": {
                                    "total": 24.409586323890835,
                                    "count": 10928,
                                    "self": 1.0102361484896392,
                                    "children": {
                                        "TorchPolicy.evaluate": {
                                            "total": 23.399350175401196,
                                            "count": 20554,
                                            "self": 23.399350175401196
                                        }
                                    }
                                },
                                "workers": {
                                    "total": 0.14480665873270482,
                                    "count": 10927,
                                    "self": 0.0,
                                    "children": {
                                        "worker_root": {
                                            "total": 758.7287535099313,
                                            "count": 10927,
                                            "is_parallel": true,
                                            "self": 164.56046863412485,
                                            "children": {
                                                "steps_from_proto": {
                                                    "total": 0.010425042011775076,
                                                    "count": 2,
                                                    "is_parallel": true,
                                                    "self": 0.0018061659066006541,
                                                    "children": {
                                                        "_process_rank_one_or_two_observation": {
                                                            "total": 0.008618876105174422,
                                                            "count": 8,
                                                            "is_parallel": true,
                                                            "self": 0.008618876105174422
                                                        }
                                                    }
                                                },
                                                "UnityEnvironment.step": {
                                                    "total": 594.1578598337946,
                                                    "count": 10927,
                                                    "is_parallel": true,
                                                    "self": 1.9479959602467716,
                                                    "children": {
                                                        "UnityEnvironment._generate_step_input": {
                                                            "total": 22.344160036183894,
                                                            "count": 10927,
                                                            "is_parallel": true,
                                                            "self": 22.344160036183894
                                                        },
                                                        "communicator.exchange": {
                                                            "total": 555.249114718521,
                                                            "count": 10927,
                                                            "is_parallel": true,
                                                            "self": 555.249114718521
                                                        },
                                                        "steps_from_proto": {
                                                            "total": 14.616589118842967,
                                                            "count": 21854,
                                                            "is_parallel": true,
                                                            "self": 3.9547483124770224,
                                                            "children": {
                                                                "_process_rank_one_or_two_observation": {
                                                                    "total": 10.661840806365944,
                                                                    "count": 87416,
                                                                    "is_parallel": true,
                                                                    "self": 10.661840806365944
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        },
                        "trainer_advance": {
                            "total": 110.53715236962307,
                            "count": 21854,
                            "self": 0.5653643566183746,
                            "children": {
                                "process_trajectory": {
                                    "total": 31.80078726389911,
                                    "count": 21854,
                                    "self": 31.80078726389911
                                },
                                "_update_policy": {
                                    "total": 78.17100074910559,
                                    "count": 48,
                                    "self": 58.156830669147894,
                                    "children": {
                                        "TorchPPOOptimizer.update": {
                                            "total": 20.014170079957694,
                                            "count": 1443,
                                            "self": 20.014170079957694
                                        }
                                    }
                                }
                            }
                        }
                    }
                },
                "trainer_threads": {
                    "total": 4.66697383671999e-06,
                    "count": 1,
                    "self": 4.66697383671999e-06
                },
                "TrainerController._save_models": {
                    "total": 0.29044020909350365,
                    "count": 1,
                    "self": 0.0022096250904724,
                    "children": {
                        "RLTrainer._checkpoint": {
                            "total": 0.28823058400303125,
                            "count": 2,
                            "self": 0.28823058400303125
                        }
                    }
                }
            }
        }
    }
}